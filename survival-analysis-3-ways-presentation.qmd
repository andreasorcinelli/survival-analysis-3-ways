---
title: "Predicting Time Between Events: Survival Analysis 3 Ways"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
```

## Who are you?

![](https://media.giphy.com/media/3ohs4lOlH8GhRtb6QU/giphy.gif)

## I'm Andrea. Hi! {.smaller}

::: columns
::: {.column width="60%"}
![](talk_images/sorcinelli_headshot.jpg)
:::

::: {.column width="40%"}
```{r}
#| eval: false
andrea |> 
  study(cognition_and_perception) |> 
  work(brain_pop, warby_parker)
```
:::
:::

## How I think about code in talks

There is code in this talk.

Don't worry about the details.

*Really.*

::: notes
Before we get started, I wanted to make a quick note. There is code in this talk. Don't really worry about the details. 

Really. 

That advice might seem a little counter-intuitive at first but . . .
:::

## My goal is for you to leave with an understanding of

-   The kinds of situations that call for survival analysis\
-   Why it is important not to treat censored observations as missing\
-   There are different packages for implementing survival analysis in R\
-   An intuition for how survival analysis works

::: notes
Here is what I want to cover today. We are going to talk about the kinds of situations that call for survival analysis and why it is important not to treat censored observations as missing. I am also going to show you a couple of different packages for implementing survival analysis in R and finally, we are going to wrap up with intuition for how survival analysis works.
:::

## Back to how I think about code in talks

Don't worry about the details of the code.

Focus on the the high-level. It's all on GitHub.

::: notes
So you'll notice that nowehere on that agenda slide did it say anything about reading code line-by-line. In general, I would say focus on the high level. 

There's a whopping 3 lines of code I am going to walk through in detail and everything is on GitHub. So really I would say focus on the high level and don't worry about the details.
:::

## Predicting time between sign up and churn {.smaller}

Imagine you work for a communication service provider like Xfinity or Waitsfield Telecom. A big part of your business is going to be focused on understanding renewals.

Who keeps their contract and who **cancels their contract or churns**? Ideally, you'd like to be able to estimate a probability that someone will remain a customer.\
\
\
Data from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) used throughout.

::: notes
Read. 

I am going to be using a dataset from Kaggle throughout but before we get into that dataset I'm going to show you some fake data that illustrate a common problem that arises. 
:::

## Some customers signed up a long time ago; others only recently

```{r}
#| include: false
fake_data <- tibble(customer = c("Alice", "Bob", "Charlie", "Dave", "Erin"),
               signup_date = lubridate::ymd(c("2019-01-01", "2020-01-01", "2021-01-01", "2022-01-01", "2022-07-01")),
               churn_date = lubridate::ymd(c("2021-03-01", "2022-02-01", NA, NA, "2023-01-01")),
               tenure =  round(as.numeric(as.duration(churn_date - signup_date), 'months'),2),
         is_churned = as.numeric(!is.na(churn_date)),
         cohort = year(signup_date))
```

```{r}
#| echo: false
fake_data %>%
  ggplot(.,  aes(y = fct_rev(customer))) +
  geom_point(aes(x = signup_date, color = "blue"), size = 3) +
  geom_point(aes(x = churn_date, color = "red"), size = 3) +
  geom_segment(aes(x = signup_date,
                   y = customer,
                   xend = coalesce(churn_date, lubridate::ymd("2023-04-01")),
                   yend = customer),
               linewidth = 1) +
  scale_colour_manual(breaks = c("blue", "red"),
                      labels = c("Signup Date", "Churn Date"),
                      values = c("blue", "red")) +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  theme(panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black")) +
  theme(legend.position = "none") +
  labs(x = "Year",
       y = "Customer",
       title = "Time from sign up to churn",
       subtitle = "We have observed churn for some customers. \nWe have not for others.",
       caption = "Charlie and Dave are censored") +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

::: notes
So here I am plotting customer's sign up and churn dates. Sign ups are indicated with blue dots are indicated with blue dots and churn with red. 

In the top row, we see customer Alice. Alice signed up for an account in 2019 and she cancelled or churned in 2021.

And in the next row, we see Bob. And Bob signed up 2020 and cancelled or churned in 2022.

And then we see Charlie in the third row. And Charlie signed up in 2021 but Charlie still has his contract. So we have not observed Charlie cancel or churn yet. 

The same thing for Dave in the fourth row.
:::

## Charlie and Dave are censored {.smaller}

We know Charlie and Dave's starting point - their signup date, but we have not observed their ending date - their churn date.

These are **censored** observations.

It can be tempting to treat these observations as missing. But they're not.\
\
We have **partial** information for Charlie and Dave.

Excluding them from the analysis will result in incorrect estimates of when customers churn.

::: notes
Charlie and Dave are examples of censored observations. 

Censoring means that we know something about a individual's starting point, in this case, their sign up date but we are missing information about their ending point, their cancellation or churn date.

A common response is to filter censored observations like these out. In other words to treat them as missing. But they're not missing. Indeed, we have partial infortmation for them.
:::

```{r read_data}
#| include: false
# download from https://www.kaggle.com/datasets/blastchar/telco-customer-churn and put in a data folder
telco_data <- read.csv("data/WA_Fn-UseC_-Telco-Customer-Churn.csv") 
```

```{r data_cleaning}
#| include: false
 telco_data_clean <- telco_data |> 
  janitor::clean_names()  |>
  mutate(event = if_else(churn == "Yes", 1, 0)) |> 
  select(customer_id, tenure, churn, event,everything())
```

## We cannot simply remove censored observations {.smaller}

```{r calc_naive_estimates}
#| echo: false
naive_estimate <- tibble(total = nrow(telco_data_clean), 
                         telco_data_clean |> 
                           count(tenure, event, name = "monthly_total")) |> 
  filter(event == 1) |> 
  mutate(running_total = cumsum(monthly_total), 
         naive_estimate = 1 - (running_total/total))
```

```{r plot_naive_estimates}
#| echo: false
naive_estimate |> 
  ggplot(aes(x = tenure, y = naive_estimate)) +
  geom_line(color = "red") +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Simply removing censored observations overestimates the \nsurvival probability.", 
       subtitle = "These estimates are incorrect.",
       x = "Time since sign up (months)", 
       y = "Percent") +
  theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

```{r naive_estimates_demo}
#| eval: false
tibble(total = nrow(telco_data_clean), 
       telco_data_clean |> 
         count(tenure, event, name = "monthly_total")) |> 
  filter(event == 1) |> 
  mutate(running_total = cumsum(monthly_total), 
         naive_estimate = 1 - (running_total/total))
```

::: notes
What happens when we filter censored observations out is that we can construct naive estimates of the survival probability, or the probability that someone will remain a customer. But these estimates are incorrect.

This naive estimate is 1 minus the running total of individuals who have churned divided by the total n in the sample. And this over-estimates the survival probability.

And this is a common pitfall. 

Individuals who are censored only contribute information for some of the time and then they need to fall out of the risk set. In other words, they need to fall out of the denominator.

But if we ignore censoring, it treats these individuals as part of the risk set for the entire time when they are not eligible to be.
:::

## We encounter censored data all the time {.smaller}

-   Time from disease onset to death
-   Time from machine creation to failure
-   Time from order to re-order
-   Time from sign up to churn

If you are asking yourself, *"how long until?"* or *"has it happened yet?"* you likely have censored data.

\
These are all examples of **right censored** data but there are other kinds.

::: notes
We encounter censored data all the time. 

The hallmark example and what the literature is named after is predicting time from disease onset to death. But other examples are time from machine creation or widgit creation to failure, time from a customer's first order to their second order, and as we see today time from sign up to churn.
::: 

## Survival analyses help us address censored data

Survival analysis provide techniques for us to take into account censoring in our data. Two common methods:

-   Kaplan Meier Estimates to estimate survival probabilities
-   Cox Proportional Hazards to quantify effects between one or more variables and estimate the probability that an individual experiences the event of interest during a small time interval

:::notes
Survival analysis provide techniques for us to take into account censoring in our data. Two I am going to focus on two common methods today Kaplan Meier Estimates to estimate survival probabilities and Cox Proportional Hazards to quantify effects between one or more variables and estimate the probability that an individual experiences the event of interest during a small time interval. 

I am going to focus on these because they map onto two fundamental functions of interest: 

The **survival probability** or the probability that an individual survives past time t 

And the **hazard function** or the probability that an individual experiences event during a small time interval 

These are both conditional on the individual surviving up until time t
:::

## What does time-to-event data look like? {.smaller}

All you need to get started are two variables:

-   time variable (units don't matter)
-   event marker (1 = event observed; 0 = censored)

```{r time_to_event_data}
telco_data_clean |> 
  select(tenure, event) |> 
  head(10)
```

:::notes 
So what does time to event data look like? At it's core, all you need are two variables. You need a time variable and you need an event marker. So in this dataset, our time variable is named tenure and it is time months. As an aside, the units don't matter. And our event marker is the event variable where 1 is coded as the event being observed and 0 is is coded as censored, so no event. 
:::

## What is the probability of surviving past a given time? {.smaller}

##### Old School: survival package

```{r km_base_surv}
library(survival)
km_base_surv <- survfit(Surv(tenure, event) ~ 1, 
                      data = telco_data_clean) |> 
  broom::tidy(km)

km_base_surv
```

:::notes
The first thing we can do is generate the overall survival curve for the entire cohort. 

I am using the survival package to do this. This is a pretty standard way to do this. This is the old school way to do this. So you can see that after just a few lines of code, we have survival probabilities. 

These represent conditional probability of surviving beyond time t, given that an individual has survived to time t - 1. 

So we would interpret these as 92.8 percent of customers will survive beyond month 2, given that that they survived to month 1.  
:::

## What is the probability of surviving past a given time? {.smaller}

```{r plot_km_base_surv}
#| echo: false
km_base_surv %>%
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_line(aes(x = time, y = estimate), color = "blue") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = "Probability of survival beyond a given time",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

**Interpretation:** The survival probability at time t is the conditional probability of surviving beyond that time, given that an individual has survived to time t - 1.

:::notes
The next thing we would typically do is plot the Kaplan-Meier curve. 

So again these are interpretted as the conditional probability of surviving beyond time t, given that an individual has survived to time t - 1.

About 70% of customers will survive past 4 years conditional that they had survived to 3 years and 11 months. 

Optional: This is a Kaplan-Meier curve and I would say it's the most common way to estimate survival times and probabilities. It non-parametric so it doesn't make any assumptions about the underlying distribution and it results in a step function, where there is a step down each time an event occurs.
:::

## Comparing Kaplan-Meier to naive estimates

```{r compare_naive_to_km_estimates}
#| echo: false
km_naive_estimates_plot <- ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_line(data = km_base_surv, aes(x = time, y = estimate, color="Kaplan-Meier")) +
  geom_line(data = naive_estimate, aes(x = tenure, y = naive_estimate, color = "Naive")) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  scale_color_manual(name="Estimate Type",
                     breaks=c("Kaplan-Meier", "Naive"),
                     values=c("Kaplan-Meier" = "blue", "Naive"="red")) +
  labs(title = "Probability of survival beyond a given time",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))

km_naive_estimates_plot
```

::: notes
You can see here that if we compare the survival probabilities provided by the Kaplan Meier estimates to our naive estimates, our naive estimates over-estimates the survival probability. We are going to dig into the math of how this works in a bit but for now the takeaway is that we have to address censored observations in time-to-event data and we cannot just filter them out. 
:::

## Do survival probabilities differ depending on the type of internet service? {.smaller}

##### Old School: survival package

```{r km_internet_service_surv}
km_internet_service_surv <- survfit(Surv(tenure, event) ~ internet_service, 
                              data = telco_data_clean) |> 
  broom::tidy(km) 

km_internet_service_surv
```

:::notes
Another thing we can do is we can stratify our estimates by variables of interest. So there is a variable in here called internet service and this tells us the type of interest service customers have or if they don't have internet service at all and maybe I have a sense that the survival probabilities differ by what type of internet service customers have. 

So again I am using the survival package or the old school way of doing this and compared to the previous this was just a one line change of code. I am swapping out internet service in the formula call. 
:::

## Do survival probabilities differ depending on the type of internet service?

```{r plot_km_internet_service_surv}
#| echo: false
km_internet_service_surv |> 
  separate(strata, into = c("internet_service"), sep=", ") |> 
  mutate_at(c("internet_service"), ~ str_trim(str_remove(.x, '^.*='), side = "right")) |> 
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_ribbon(aes(x = time,
                  ymin = conf.low,
                  ymax = conf.high,
                  fill = internet_service),
              alpha = 0.25) +
  geom_line(aes(x = time, y = estimate, color = internet_service)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = "Probability of survival by internet service type",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

:::notes
When we plot those Kaplan-Meier estimates stratified by internet service type we can see that Fiber Optic has the lowest survival probability, DSL is in the middle, and folks who don't have internet service have the highest survival probability. 

This was a little surprising to me at first but maybe this actually makes sense. So folks who are in this internet service is "no" group only have phone service. And you can imagine scenarios wherein if you move, maybe internet from that provider is not available in your new location but you can probably take your phone service with you. So what this is telling me is that folks who have phone service with this provider are more likely to remain customers than folks who have DSL or Fiber Optic internet.   
:::

## How can we quantify the difference between groups? {.smaller}

##### Old School: survival package

```{r cox_ph_internet_service_surv}
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean) 

cox_ph_internet_service_surv
```

:::notes
Ok so based on the Kaplan Meier curves, we can see that the survival probabilities differ by internet service type. What if we wanted to quantify that difference?

Here I am showing you a call for a Cox Proportional Hazards which is a semi-parametric method. I want to draw your attention to couple of things. 

First, you'll notice we have coefficients for internet_service_FiberOptic and internet_service_No. We have 3 levels of internet service and if we don't specify a reference group R is going to default to alphabetical here. 

So DSL is the reference group here. So this first row is showing the effect of DSL compare to Fiber Optic and the second row is showing the effect of DSL compared to No internet service. 

The other thing I want to draw your attention to is that we have both a coefficient and the exponentiated coefficient in the output. We will come back to this in a moment. 

Again, I am using the survival package to do this. There is absolutely nothing wrong with this implementation but there are more modern ways to execute this same thing. 

Optional: Talk about assumptions. 
Independence of survival times between distinct individuals in the sample,
Multiplicative relationship between the predictors and the hazard.
Constant hazard ratio over time.

Non-parametric meaning it makes a parametric assumption concerning the effect of the predictors on the hazard function, but makes no assumption regarding the nature of the hazard function.
:::

## How can we quantify the difference between groups? {.smaller}

##### New School: censored package

```{r cox_ph_internet_service_tidy}
library(parsnip)
library(censored)

cox_ph_internet_service_tidy <- proportional_hazards()  |>  
  set_engine("survival") |>  
  set_mode("censored regression") |>  
  fit(
    Surv(tenure, event) ~ internet_service,
    data = telco_data_clean
)

cox_ph_internet_service_tidy
```
:::notes
So here, I am running the exact same Cox regression but I am doing so using the censored package.
The censored package is a parsnip extension package. 

parsnip is a part of the TidyModels ecosystem. parnsip is specifically designed to standardize how you fit models. 

If you use R you have probably encountered this problem that there are loads of interfaces for specifying model fit depending what package you're using.

TidyModels is really seeking to solve that problem by giving R users consistency in how you fit models and make predictions as well as the object you get back.

So while you might look at this implementation which does the exact same thing as the survival package and think hey that looks a little verbose, I just want to fit a regression. I would agree but I think that where TidyModels really shines is by giving you the flexibility to toggle seamlessly between different model types. 

So for example, if you wanted to switch this model to a regularized version, it would be one line of code change in censored changing the engine, but if if you wanted to do the same thing outside of censored, you would use the glmnet package which doesn't have a formula interface for specifying models so that becomes much clunkier. 

My heuristic is that if you just want to fit a Cox model and you are just starting to explore your data, I would start with the survival package but if you think you're going to be toggling between several different model types, then censored would be a good option to check out. 

Optional: 
survival function 
the engine is using the survival package
the mode is the specification - not fit yet

:::

## What *exactly* is a hazard of churning? {.smaller}

**Interpretation:** We are often interested in a **hazard ratio**.

A **hazard** represents the probability of churning in time t + 1 if a customer has not churned as of time t.

The **hazard ratio** therefore represents the ratio of hazards between two groups at any particular time.

-   exponentiate the regression parameter β to get the hazard ratio

-   β \< 0: HR \< 1: reduced hazard of churn

-   β \> 0: HR \> 1: increased hazard of churn

:::notes
Ok so we saw that the survival package and the censored package show us the same results for that Cox Proportional Hazards but how do we interpret those results? 

Well are often interested in two things, the hazard and the hazard ratio. 

The **hazard** represents the probability of churning in time t + 1 if a customer has not churned as of time t.

And the  **hazard ratio** is the ratio of hazards between two groups at any particular time.

We saw the hazard ratio in the outputs from the Cox model. Recall I mentioned we had both the coefficient and the exponentiated coefficient. The latter is the hazard ratio. 

So a coefficient of less than 0, when we exponentiate it, it will be a hazard ratio of less than 1 which is interpreted as a reduced hazard of churning. 

Conversely, a coefficient that greater than 0 becomes a hazard ratio that's greater than 1 which is interpreted as an increased hazard of churn.
:::

## What *exactly* is a hazard of churning? {.smaller}

So the HR = 2.21 means that 2.21 times as many customers with Fiber Optic are churning than those with DSL at any given time. **In other words, Fiber Optic has a significantly higher hazard of churn than DSL.**

And the HR = 0.40 implies that 0.40 times as many customers no internet service are churning as those with with DSL at any given time. **In other words, customers with no internet service have a significantly lower hazard of churn than those with DSL.**

```{r}
#| echo: false
cox_ph_internet_service_tidy
```

:::notes
Going back to our example of internet service type. 

The HR of 2.21 means that 2.21 times as many customers with Fiber Optic are churning than those with DSL at any given time. **In other words, Fiber Optic has a significantly higher hazard of churn than DSL.**

And the HR of 0.40 implies that 0.40 times as many customers no internet service are churning as those with with DSL at any given time. **In other words, customers with no internet service have a significantly lower hazard of churn than those with DSL.**
:::

## Instantaneous hazard of churning

```{r extract_hazards_and_plot}
#| echo: false
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean)

pred_df <- cross_df(
  list(strata = unique(telco_data_clean[!is.na(telco_data_clean[["internet_service"]]), ][["internet_service"]]),
       tenure = 0:72,
       event = 1)) |> 
  rename("internet_service" = strata) |> 
  arrange(internet_service, tenure)

pred_df <- pred_df |>
  mutate(estimate = predict(cox_ph_internet_service_surv, pred_df, type = "survival")) |>
  rename(time = tenure)

pred_df |>  
  group_by(internet_service) |> 
  mutate(hazard = c(NA, -diff(estimate)) / estimate) |> 
  ungroup() |> 
  ggplot(aes(x = time, y = hazard, color = internet_service)) +
  geom_line() +
  scale_x_continuous(name = "Time since sign up",
                     breaks = seq(0, 72, by = 6)) +
  scale_y_continuous(labels = scales::percent, name = "Probability") +
  theme_minimal()

```

:::notes
One of the other interesting things we can do is take a look at the hazards or the probability of churning in the next instant. So this is the probability of churning in time t + 1 given that a customer has not churned as of time t.

We can notice a couple of things. So first the order of the curves is going to be flipped relative to the Kaplan Meier curves and that intuitively maps onto what we learned about the hazard ratios. 

Fiber Optic has a higher hazard ratio relative to DSL at any given time and DSL has a higher hazard ratio than no internet service at any given time. 
:::

## Cool but how does it work?

![](https://media.giphy.com/media/yj5oYHjoIwv28/giphy.gif)

## Intuition for Kaplan-Meier estimator {.smaller}

$$\widehat{S}(t)∏_{i: t_{i}≤t}\left(1 - \frac{d_{i}}{n_{i}}\right)$$

-   Survival probability at time t is conditional probability of surviving beyond t, given survival to t - 1.

-   Divide the number of customers who have renewed excluding censoring at time t by the number of customers who had renewed at time t -1.

-   The Kaplan-Meier estimate of survival time t is product of all the conditional probabilities up until t.

::: notes
We can estimate the survival probability by dividing the number of customers who have survived excluding censoring at that time, divided by the number of customers who had survived just prior to that time.

The Kaplan-Meier estimate of survival time t is the product of all the conditional probabilities up until time t.
:::

## Intuition for Kaplan-Meier estimator {.smaller}

##### My School: Let's play around

```{r make_km_estimates_by_hand}
#| code-line-numbers: "|3,4|6,7,8,9,10,11|12,13,14,15"
make_km_estimates_by_hand <- function(df, time_var, event_var){
  
  total_at_start <- df |> 
    nrow()
  
  df |> 
    group_by({{ time_var }}) %>% 
    summarise(
      n_event = sum({{ event_var }}), 
      n_censor = sum({{ event_var }} == 0)
      ) |> 
    mutate(
      n_risk = lag(total_at_start - cumsum(n_event) - cumsum(n_censor), default = total_at_start),
      surv_prob = (n_risk - n_event)/n_risk, 
      estimate = cumprod((n_risk - n_event)/n_risk), 
      std_error = sqrt(cumsum(n_event/(n_risk*(n_risk - n_event)))) * estimate,
      conf_high = estimate + (1.96 * std_error), 
      conf_low = estimate - (1.96 * std_error)
      ) |> 
    select(
      "time" = {{ time_var}}, 
       n_risk, 
       n_event, 
       n_censor, 
       surv_prob,
       estimate, 
       std_error, 
       conf_high, 
       conf_low)
}
```

::: notes
Recall the key thing we need to do is understand what the risk set is at each point in time. If you have already churned in month 3, you are not at risk to churn in month 36. So you need to be removed from the risk set. 

So how are we going to do this? Here is a function that I have written to break this down step by step. You'll notice this function takes 3 arguments a dataframe, a time variable, and an event variable. 

ANIMATE
We start by calculating the total number in the risk set at the start. This is the total number of customers in the sample, which I am assigning to total at start.  

ANIMATE
Next we are going to group by our unit of time, which in this case is month, and sum up how many customers had the event of interest, i.e. how mnay churned and how many were censored. We are going to use these later to update our risk set.    

ANIMATE
So remember how I said there were 3 lines of code that were really fundamental to what we are talking about? These are those them. 

In this mutate statement, I am taking the number of customers at start for each month subtracting the number of customers who churned or were censored each month. We are removing these observations because they are ineligible to be in the risk set going forward. If you have already churned or were censored at month 3, you are not at risk at month 12.  

In this way, we are iteratively updating the risk set each month. 

Next we calculate the survival probability at each time point which is the number at risk minus the number who had the event divided by the number at risk. 

And then finally to calculate the conditional probability of surviving beyond time t we take the product of all those conditional probabilities and that is out Kaplan-Meier estimate. 
:::

## Intuition for Kaplan Meier estimator {.smaller}

##### My School: What does that output look like?

```{r}
km_base_as <- make_km_estimates_by_hand(telco_data_clean, tenure, event)
km_base_as
```

:::notes
So let's look at the output. We had 7043 customers at start. For each month, we subtract the cumulative sums of customers who either churned or were censored to iteratively update that risk set. 

We divide the number at risk minus the number who had the event by the number at risk to get the conditional probabilities for each month and then we take the product of those conditional probabilities to get our Kaplan-Meier estimates. 

This output is identical to that from the survival package with the exception that I am printing one extra column here for the sake of example. But this is it. We did it. 
:::

## Comparing the 3 implementations {.smaller}

| Old School: survival package                            | New School: censored package                                            | My School: custom function                       |
|---------------------------|-------------------|--------------------------|
| Just a few lines of code; quick and easy to get started | A little more verbose initially                                         | Dear god, don't ever use this in production code |
|                                                         | Consistent experience fitting, predicting, and the object you get back | Helpful for giving an intuition                  |

## Don't simply exclude censored observations {.smaller}

```{r compare_naive_to_km_estimates_repeat}
#| echo: false
km_naive_estimates_plot
```

It is essential to address **censoring** when you are working with time-to-event data. **Survival analyses** are a use set of tools. There are a few packages for implementing survival analysis in R.

## Special topics {.smaller}

-   **competing risks** - occurs when there are multiple possible alternative events
    -   death from disease
    -   death from other causes
    -   treatment response
-   **predicting recurrent events** - occurs when events may be repeated
    -   disease recurrence
    -   repurchases
-   **interval censored data** - occurs when we know that an event took place during an interval but not the exact time
    -   dentistry
    -   ophthalmology
-   **synthesizing data** - method for simulating time-to-event data without specifying a particular distribution for the baseline hazard function

## Additional resources

[Emily Zabor Survival Analysis Tutorial](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Part_1:_Introduction_to_Survival_Analysis)

[Hannah Frick's censored - survival analysis in Tidymodels from rstudio_conf(2022)](https://www.rstudio.com/conference/2022/talks/censored-survival-analysis-in-tidymodels/)

[censored - parsnip extension package providing engines for censored regression and survival analysis](https://censored.tidymodels.org/)

[Red Door Analytics Survival Analysis Tutorials](https://reddooranalytics.se/resources/tutorials/)

[coxed package - for simulating survival data](https://cran.r-project.org/web/packages/coxed/vignettes/simulating_survival_data.html)

[ggsurvfit package - for plotting time-to-event data including risk tables](http://www.danieldsjoberg.com/ggsurvfit/index.html)
