---
title: "Predicting Time Between Events: Survival Analysis 3 Ways"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
```

## Outline {.smaller}

-   [x] Intro and background about me

-   [x] Notes about how I present

-   [x] There's code on the slides

    -   [x] Don't worry about reading it.

-   [ ] Teach you how to drive the car, then show you what's under the hood.

-   [x] Motivating problem - censored data

-   [x] Where might you use this?

    -   [x] Anytime you are predicting time to an event (churn, repurchase, widget failure)

-   [ ] What is survival analysis?

-   [x] Kaplan Meier Estimates in Survival package

-   [x] Plotting Kaplan Meier curves

-   [ ] Comparing groups using Cox Ph Survival package

-   [ ] Tidymodels - censored - parsnip extension package

-   [ ] Intuition for KM estimators

-   [ ] What does each approach afford you?

-   [ ] Return to motivating slides with biased and unbiased estimaes

-   [x] Advanced topics

-   [x] References

## Choices {.smaller}

-   How do I want to show naive estimates? I don't think I want to show them in the initial set up. I don't think that has been as intuitive for people.
-   I'm not sure I want to show median time to churn by month
-   I think I want to show overall probability of survival with censoring and without censoring on the same graph.
-   Unresolved questions: do I want to show ggsurvfit for making plots? I tend not to do this but it might be an easy way for folks to get started.
-   Do I want to plot hazards?
-   Do I want to show extracting predictions from a cox surv object?

## Who are you?

![](https://media.giphy.com/media/3ohs4lOlH8GhRtb6QU/giphy.gif)

## I'm Andrea. Hi!

![](talk_images/sorcinelli_headshot.jpg)

```{r}
#| eval: false
andrea |> 
  study(cognition_and_perception) |> 
  work(brain_pop, warby_parker)
```

## I'm Andrea. Hi!

::: {layout-ncol="2"}
![](talk_images/sorcinelli_headshot.jpg){height="300%"}

```{r}
#| eval: false
andrea |> 
  study(cognition_and_perception) |> 
  work(brain_pop, warby_parker)
```
:::

## How I present code in talks

There is code in this talk.

Don't worry about the details.

*Really.*

## My goal is for you to leave with an understanding of

-   The kinds of situations that call for survival analysis\
-   Why it is important not to treat censored observations as missing\
-   There are different packages for implementing survival analysis in R\
-   An intuition for how survival analysis works

## Back to how I present code in talks

Don't worry about the specifics of the code.

Focus on the the high-level. It's all on GitHub.

There are 4 lines of code that we are going to go through in detail.

## Predicting time between sign up and churn {.smaller}

Imagine you work for a communication service provider like Xfinity or Verizon. A big part of your business is going to be focused on understanding renewals. Who keeps their contract and who **cancels their contract or churns**?

Ideally, you'd like to be able to predict a probability of churning.

Data from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) used throughout.

## Some customers signed up a long time ago; others only recently

```{r}
#| include: false
fake_data <- tibble(customer = c("Alice", "Bob", "Charlie", "Dave", "Erin"),
               signup_date = lubridate::ymd(c("2019-01-01", "2020-01-01", "2021-01-01", "2022-01-01", "2022-07-01")),
               churn_date = lubridate::ymd(c("2021-03-01", "2022-02-01", NA, NA, "2023-01-01")),
               tenure =  round(as.numeric(as.duration(churn_date - signup_date), 'months'),2),
         is_churned = as.numeric(!is.na(churn_date)),
         cohort = year(signup_date))
```

```{r}
#| echo: false
fake_data %>%
  ggplot(.,  aes(y = fct_rev(customer))) +
  geom_point(aes(x = signup_date, color = "blue"), size = 3) +
  geom_point(aes(x = churn_date, color = "red"), size = 3) +
  geom_segment(aes(x = signup_date,
                   y = customer,
                   xend = coalesce(churn_date, lubridate::ymd("2023-04-01")),
                   yend = customer),
               linewidth = 1) +
  scale_colour_manual(breaks = c("blue", "red"),
                      labels = c("Signup Date", "Churn Date"),
                      values = c("blue", "red")) +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  theme(panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black")) +
  theme(legend.position = "none") +
  labs(x = "Year",
       y = "Customer",
       title = "Time from sign up to churn",
       subtitle = "We have observed churn for some customers. \nWe have not for others.",
       caption = "Charlie and Dave are censored") +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

## Charlie and Dave are censored {.smaller}

We know Charlie and Dave's starting point - their signup date, but we have not observed their ending date - their churn date.

These are **censored** observations.

It can be tempting to treat these observations as missing. But they're not.\
We have partial information for Charlie and Dave and excluding them from the analysis will result in incorrect estimates of when customers churn.

## We encounter censored data all the time {.smaller}

-   Time from disease onset to death
-   Time from widget creation to failure
-   Time from order to re-order

If you are asking yourself, *"how long until?"* or *"has it happened yet?"* you likely have censored data. These are all examples of right censored data but there are other kinds.

## Survival analyses help us address censored data

```{r read_data}
#| include: false
# download from https://www.kaggle.com/datasets/blastchar/telco-customer-churn and put in a data folder
telco_data <- read.csv("data/WA_Fn-UseC_-Telco-Customer-Churn.csv") 
```

```{r preprocess_time_vars}
#| include: false
set.seed(502)

telco_data_clean <- telco_data |> 
  janitor::clean_names()  |>
  # Assuming that tenure is in months 
  # I want to create a signup date at a finer grain than the month but I don't really have that information. 
  # So I randomly sample days between 0 and 30. 
  # I know I should take into account months with greater than or less than 30 days but not in scope for this talk :)
  # This is a little strange bc it's going to artificially set the signup date to the same day each month but I don't have data at a finer grain. 
  mutate(signup_date = today() - months(tenure), 
         signup_day = sample(1:28, dim(telco_data)[1], replace = TRUE),
         signup_month = if_else(month(signup_date) > 9, 
                               as.character(month(signup_date)), 
                               paste0("0", month(signup_date))),
         #signup_month = ym(paste0(year(signup_date), signup_month)),
         signup_date = ymd(paste0(year(signup_date), signup_month, signup_day)),
         month = ymd(sprintf("%d-%0d-01",
                             year(signup_date),
                             month(signup_date))),
         event = if_else(churn == "Yes", 1, 0)) |> 
    select(customer_id, tenure, churn, event, signup_date, signup_month, month, everything())
```

```{r calc_biased_monthly_churn}
#| include: false
monthly_stats <- telco_data_clean |> 
  count(month, name = "total") |> 
  left_join(telco_data_clean |> 
              count(month, churn), by = "month") |> 
  mutate(percent = n/total) |> 
  left_join(telco_data_clean |>
              group_by(month) |>
              summarize(months_to_churn = median(tenure / 30, na.rm = TRUE), .groups = "drop"), 
            by = "month") |> 
  select(month, total, churn, n, percent, months_to_churn)
```

```{r plot_biased_monthly_churn}
#| echo: false
monthly_stats |> 
  filter(churn == "No", month != max(month)) |> 
  ggplot(aes(x = month, y = percent)) +
  geom_line(color = "blue") +
  scale_x_date(name = "Signup Date") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "TODO: Add all the stuff", 
       subtitle = "") +
  theme_minimal()
```

## What is the probability of churning by time since sign up? {.smaller}

##### Old School: Survival Package

```{r km_base_surv}
library(survival)
# Kaplan Meier Estimates
km_base_surv <- survfit(Surv(tenure, event) ~ 1, 
                      data = telco_data_clean) |> 
  broom::tidy(km)

km_base_surv
```

## Kaplan Meier curve

```{r plot_km_base_surv}
#| echo: false
km_base_surv %>%
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_ribbon(aes(x = time,
                  ymin = conf.low,
                  ymax = conf.high),
              alpha = 0.25) +
  geom_line(aes(x = time, y = estimate)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = 'Probability of churning by months since sign up',
       subtitle = '',
       caption = '',
       x = 'Months since sign up',
       y = 'Probability') +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

## Does probability of churning differ depending on the type of internet service? {.smaller}

##### Old School: Survival Package

```{r km_internet_service_surv}
km_internet_service_surv <- survfit(Surv(tenure, event) ~ internet_service, 
                              data = telco_data_clean) |> 
  broom::tidy(km) 

km_internet_service_surv
```

## Kaplan Meier curves by internet service type

```{r plot_km_internet_service_surv}
#| echo: false
km_internet_service_surv |> 
  separate(strata, into = c("internet_service"), sep=', ') |> 
  mutate_at(c("internet_service"), ~ str_trim(str_remove(.x, '^.*='), side = "right")) |> 
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_ribbon(aes(x = time,
                  ymin = conf.low,
                  ymax = conf.high,
                  fill = internet_service),
              alpha = 0.25) +
  geom_line(aes(x = time, y = estimate, color = internet_service)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = 'Probability of churning by months and internet service type',
       subtitle = '',
       caption = '',
       x = 'Months since sign up',
       y = 'Probability') +
    theme_minimal()
```

## How can we quantify the difference between groups? {.smaller}

##### Old School: Survival Package

```{r}
# Cox Proportional Hazards Survival Package
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean %>%
        # because the default for factor levels is alphabetical, the enabled becomes the reference group
        # unless we reorder the levels.
        mutate(internet_service = factor(internet_service, levels = c("Fiber optic", "DSL", "No")))) %>%
  broom::tidy()

cox_ph_internet_service_surv
```

## Cox Proportions Hazards in TidyModels {.smaller}

##### New School: Censored Package

```{r cox_ph_internet_service_tidy}
library(parsnip)
library(censored)

cox_ph_internet_service_tidy <- proportional_hazards()  |>  
  set_engine("survival") |>  
  set_mode("censored regression") |>  
  fit(
    Surv(tenure, event) ~ internet_service,
    data = telco_data_clean
)

cox_ph_internet_service_tidy
```

## Cool but how do they work? {.smaller}

##### My School: Let's play around

```{r make_km_estimates_by_hand}

make_km_estimates_by_hand <- function(df, time_var, event_var){
  
  total_at_start <- df |> 
    nrow()
  
  df |> 
    group_by({{ time_var }}) %>% 
    summarise(n_event = sum({{ event_var }}), 
              n_censor = sum({{ event_var }} == 0)) |> 
    mutate(n_risk = lag(total_at_start - cumsum(n_event) - cumsum(n_censor), default = total_at_start),
           prob_repurchase = (n_risk - n_event)/n_risk, 
           estimate = cumprod((n_risk - n_event)/n_risk), 
           std_error = sqrt(cumsum(n_event/(n_risk*(n_risk - n_event)))) * estimate,
           conf_high = estimate + (1.96 * std_error), 
           conf_low = estimate - (1.96 * std_error)) |> 
    select("time" = {{ time_var}}, 
           n_risk, 
           n_event, 
           n_censor, 
           estimate, 
           std_error, 
           conf_high, 
           conf_low)
}
```

```{r}
#| include: false
km_base_as <- make_km_estimates_by_hand(telco_data_clean, tenure, event)
km_base_as
```

```{r}
#| include: false
# pulled from here
# https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/bs704_survival4.html#:~:text=With%20the%20Kaplan%2DMeier%20approach,the%20actuarial%20life%20table%20approach.
# I don't agree that there are 20 observations at risk at start
# I see 18

simple_df <- tibble(time = c(0,1,2,3,5,6,9,10,11,12,13,14,17,18,19,21,23,24), 
                   event= c(0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,0,1,0))

make_km_estimates_by_hand(simple_df, time, event) |> 
    mutate(estimate = round(estimate,3),
         std_error =round(std_error,3))
```

```{r}
#| include: false
# pulled from here
# https://real-statistics.com/survival-analysis/kaplan-meier-procedure/confidence-interval-for-the-survival-function/
another_simple_df <- tibble(time = c(3,5,8,10,5,5,8,12,15,14,2,11,10,9,12,5,8,11), 
                   event = c(1,1,1,0,1,0,1,1,0,0,1,1,0,0,1,1,1,1))

make_km_estimates_by_hand(another_simple_df, time, event)
```

## Placeholder

TODO: Verbally explain the math. Something like this but simpler. The survival probability at time t is the conditional probability of surviving beyond that time, given that an individual has survived to time t - 1.

The survival probability can be estimated as the number of customers who have renewed excluding censoring at that time, divided by the number of customers who had renewed just prior to that time.

The Kaplan-Meier estimate of survival time t is the product of all the conditional probabilities up until time t.

## Placeholder
TODO: Go back to plot comparing naive estimates vs. those from Kaplan Meier. 

## Comparing the 3 Implementations

| Old School: survival package                          | New School: censored package | My School: custom function                       |
|----------------------------|------------------|--------------------------|
|                                                      |                              | Dear god, don't ever use this in production code |
| Extracting predictions is a little more of a headache |                              | Helpful for giving an intuition                  |
|                                                       |                              |                                                  |

## Special Topics

-   competing risks
-   predicting recurrent events
-   synthesizing data

## References

[Emily Zabor Survival Analysis Tutorial](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Part_1:_Introduction_to_Survival_Analysis)

[Hannah Frick's censored - survival analysis in Tidymodels from rstudio_conf(2022)](https://www.rstudio.com/conference/2022/talks/censored-survival-analysis-in-tidymodels/)

[censored - parsnip extension package](https://censored.tidymodels.org/)
