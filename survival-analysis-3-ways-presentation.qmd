---
title: "Predicting Time Between Events: Survival Analysis 3 Ways"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
```

## Outline {.smaller}

-   [x] Intro and background about me

-   [x] Notes about how I present

-   [x] There's code on the slides

    -   [x] Don't worry about reading it.

-   [ ] Teach you how to drive the car, then show you what's under the hood.

-   [x] Motivating problem - censored data

-   [x] Where might you use this?

    -   [x] Anytime you are predicting time to an event (churn, repurchase, widget failure)

-   [x] What is survival analysis?

-   [x] Kaplan Meier Estimates in Survival package

-   [x] Plotting Kaplan Meier curves

-   [x] Comparing groups using Cox Ph Survival package

-   [x] Tidymodels - censored - parsnip extension package

-   [x] Intuition for KM estimators

-   [x] What does each approach afford you?

-   [x] Return to motivating slides with naive and km estimaes

-   [x] Advanced topics

-   [x] References

## TODOs {.smaller}

-   [ ] Clean up dead code - lung and simple dfs at end
-   [x] WTF is up with the naive estimates plot?
-   [ ] Figure out speaker notes
-   [x] Plot naive estimates and KM curves on same plot.
-   [x] Add custom legend to this plot.
-   [ ] You can use color more effectively throughout - figure out parallel construction for color over all plots
-   [x] Add ggsurvfit for making plots to refs
-   [ ] Give slide titles and plot titles another pass
-   [ ] The Intuition Slide needs a visual - the text at present is more like the speaker notes
-   [x] Decide: Do I want to plot hazards?
-   [x] Decide: Do I want to show extracting predictions from a cox surv object? It seems a little weird that I mention this is more involved but don't show it. Alternatively, I may include the code chunk but keep it hidden so that folks could look it up on GitHub if they want. Not sure.

## Who are you?

![](https://media.giphy.com/media/3ohs4lOlH8GhRtb6QU/giphy.gif)

## I'm Andrea. Hi!

![](talk_images/sorcinelli_headshot.jpg)

```{r}
#| eval: false
andrea |> 
  study(cognition_and_perception) |> 
  work(brain_pop, warby_parker)
```

## I'm Andrea. Hi! {.smaller}

::: columns
::: {.column width="60%"}
![](talk_images/sorcinelli_headshot.jpg)
:::

::: {.column width="40%"}
```{r}
#| eval: false
andrea |> 
  study(cognition_and_perception) |> 
  work(brain_pop, warby_parker)
```
:::
:::

## How I present code in talks

There is code in this talk.

Don't worry about the details.

*Really.*

## My goal is for you to leave with an understanding of

-   The kinds of situations that call for survival analysis\
-   Why it is important not to treat censored observations as missing\
-   There are different packages for implementing survival analysis in R\
-   An intuition for how survival analysis works

## Back to how I present code in talks

Don't worry about the details of the code.

Focus on the the high-level. It's all on GitHub.

## Predicting time between sign up and churn {.smaller}

Imagine you work for a communication service provider like Xfinity or Waitsfield Telecom. A big part of your business is going to be focused on understanding renewals.

Who keeps their contract and who **cancels their contract or churns**? Ideally, you'd like to be able to estimate a probability of renewal.\
\
\
Data from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) used throughout.

## Some customers signed up a long time ago; others only recently

```{r}
#| include: false
fake_data <- tibble(customer = c("Alice", "Bob", "Charlie", "Dave", "Erin"),
               signup_date = lubridate::ymd(c("2019-01-01", "2020-01-01", "2021-01-01", "2022-01-01", "2022-07-01")),
               churn_date = lubridate::ymd(c("2021-03-01", "2022-02-01", NA, NA, "2023-01-01")),
               tenure =  round(as.numeric(as.duration(churn_date - signup_date), 'months'),2),
         is_churned = as.numeric(!is.na(churn_date)),
         cohort = year(signup_date))
```

```{r}
#| echo: false
fake_data %>%
  ggplot(.,  aes(y = fct_rev(customer))) +
  geom_point(aes(x = signup_date, color = "blue"), size = 3) +
  geom_point(aes(x = churn_date, color = "red"), size = 3) +
  geom_segment(aes(x = signup_date,
                   y = customer,
                   xend = coalesce(churn_date, lubridate::ymd("2023-04-01")),
                   yend = customer),
               linewidth = 1) +
  scale_colour_manual(breaks = c("blue", "red"),
                      labels = c("Signup Date", "Churn Date"),
                      values = c("blue", "red")) +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  theme(panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black")) +
  theme(legend.position = "none") +
  labs(x = "Year",
       y = "Customer",
       title = "Time from sign up to churn",
       subtitle = "We have observed churn for some customers. \nWe have not for others.",
       caption = "Charlie and Dave are censored") +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

## Charlie and Dave are censored {.smaller}

We know Charlie and Dave's starting point - their signup date, but we have not observed their ending date - their churn date.

These are **censored** observations.

It can be tempting to treat these observations as missing. But they're not.\
\
We have partial information for Charlie and Dave.

Excluding them from the analysis will result in incorrect estimates of when customers churn.

```{r read_data}
#| include: false
# download from https://www.kaggle.com/datasets/blastchar/telco-customer-churn and put in a data folder
telco_data <- read.csv("data/WA_Fn-UseC_-Telco-Customer-Churn.csv") 
```

```{r data_cleaning}
#| include: false
 telco_data_clean <- telco_data |> 
  janitor::clean_names()  |>
  mutate(event = if_else(churn == "Yes", 1, 0)) |> 
  select(customer_id, tenure, churn, event,everything())
```

## We cannot simply remove censored observations

```{r calc_naive_estimates}
#| echo: false
naive_estimate <- tibble(total = nrow(telco_data_clean), 
                         telco_data_clean |> 
                           count(tenure, event, name = "monthly_total")) |> 
  filter(event == 1) |> 
  mutate(running_total = cumsum(monthly_total), 
         naive_estimate = 1 - (running_total/total))
```

```{r plot_naive_estimates}
#| echo: false
naive_estimate |> 
  ggplot(aes(x = tenure, y = naive_estimate)) +
  geom_line(color = "red") +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Simply removing censored observations over-estimates the \nsurvival probability.", 
       subtitle = "These estimates are incorrect.",
       x = "Time since sign up (months)", 
       y = "Percent") +
  theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

## We encounter censored data all the time {.smaller}

-   Time from disease onset to death
-   Time from machine creation to failure
-   Time from order to re-order

If you are asking yourself, *"how long until?"* or *"has it happened yet?"* you likely have censored data.

\
These are all examples of **right censored** data but there are other kinds.

## Survival analyses help us address censored data

Survival analysis provide techniques for us to take into account censoring in our data. Two common methods:

-   Kaplan Meier Estimates to estimate survival probabilities
-   Cox Proportional Hazards to quantify effects between one or more variables

## What is the probability of renewing past a given time? {.smaller}

##### Old School: survival package

```{r km_base_surv}
library(survival)
km_base_surv <- survfit(Surv(tenure, event) ~ 1, 
                      data = telco_data_clean) |> 
  broom::tidy(km)

km_base_surv
```

## What is the probability of renewing past a given time? {.smaller}

```{r plot_km_base_surv}
#| echo: false
km_base_surv %>%
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  # geom_ribbon(aes(x = time,
  #                 ymin = conf.low,
  #                 ymax = conf.high,
  #                 color = "blue"),
  #             alpha = 0.25) +
  geom_line(aes(x = time, y = estimate), color = "blue") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = "Probability of renewal beyond a given time",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

**Interpretation:** The survival probability at time t is the conditional probability of surviving beyond that time, given that an individual has survived to time t - 1.

## Comparing Kaplan Meier to naive estimates

```{r compare_naive_to_km_estimates}
#| echo: false
km_naive_estimates_plot <- ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_line(data = km_base_surv, aes(x = time, y = estimate, color="Kaplan Meier")) +
  geom_line(data = naive_estimate, aes(x = tenure, y = naive_estimate, color = "Naive")) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  scale_color_manual(name="Estimate Type",
                     breaks=c("Kaplan Meier", "Naive"),
                     values=c("Kaplan Meier" = "blue", "Naive"="red")) +
  labs(title = "Probability of renewal beyond a given time",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))

km_naive_estimates_plot
```

## Do survival probabilities differ depending on the type of internet service? {.smaller}

##### Old School: survival package

```{r km_internet_service_surv}
km_internet_service_surv <- survfit(Surv(tenure, event) ~ internet_service, 
                              data = telco_data_clean) |> 
  broom::tidy(km) 

km_internet_service_surv
```

## Do survival probabilities differ depending on the type of internet service?

```{r plot_km_internet_service_surv}
#| echo: false
km_internet_service_surv |> 
  separate(strata, into = c("internet_service"), sep=', ') |> 
  mutate_at(c("internet_service"), ~ str_trim(str_remove(.x, '^.*='), side = "right")) |> 
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_ribbon(aes(x = time,
                  ymin = conf.low,
                  ymax = conf.high,
                  fill = internet_service),
              alpha = 0.25) +
  geom_line(aes(x = time, y = estimate, color = internet_service)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = 'Probability of churning by months and internet service type',
       subtitle = '',
       caption = '',
       x = 'Time since sign up (months)',
       y = 'Probability') +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

## How can we quantify the difference between groups? {.smaller}

##### Old School: survival package

```{r cox_ph_internet_service_surv}
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean) 

cox_ph_internet_service_surv
```

## How can we quantify the difference between groups? {.smaller}

##### New School: censored package

```{r cox_ph_internet_service_tidy}
library(parsnip)
library(censored)

cox_ph_internet_service_tidy <- proportional_hazards()  |>  
  set_engine("survival") |>  
  set_mode("censored regression") |>  
  fit(
    Surv(tenure, event) ~ internet_service,
    data = telco_data_clean
)

cox_ph_internet_service_tidy
```

## What *exactly* is a hazard of churning? {.smaller}

**Interpretation:** We are often interested in a **hazard ratio**.

A **hazard** represents the probability of churning in time t + 1 if a customer has not churned as of time t.

The **hazard ratio** therefore represents the ratio of hazards between two groups at any particular time.

-   exponentiate the regression parameter β to get the hazard ratio

-   β \< 0: HR \< 1: reduced hazard of churn

-   β \> 0: HR \> 1: increased hazard of churn

## What *exactly* is a hazard of churning? {.smaller}

So the HR = 2.21 means that 2.21 times as many customers with Fiber Optic are churning than those with DSL at any given time. **In other words, DSL has a significantly lower hazard of churn than Fiber Optic.**

And the HR = 0.40 implies that 0.40 times as many customers no internet service are churning as those with with DSL at any given time. **In other words, customers with DSL have a significantly higher hazard of churn than those with no internet service.**

```{r}
#| echo: false
cox_ph_internet_service_tidy
```

## Plot the hazards

```{r extract_hazards_and_plot}
#| echo: false
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean)

pred_df <- cross_df(
  list(strata = unique(telco_data_clean[!is.na(telco_data_clean[["internet_service"]]), ][["internet_service"]]),
       tenure = 0:72,
       event = 1)) |> 
  rename("internet_service" = strata) |> 
  arrange(internet_service, tenure)

pred_df <- pred_df |>
  mutate(estimate = predict(cox_ph_internet_service_surv, pred_df, type = "survival")) |>
  rename(time = tenure)

pred_df |>  
  group_by(internet_service) |> 
  mutate(hazard = c(NA, -diff(estimate)) / estimate) |> 
  ungroup() |> 
  ggplot(aes(x = time, y = hazard, color = internet_service)) +
  geom_line() +
  scale_x_continuous(name = "Time since sign up purchase",
                     breaks = seq(0, 72, by = 6)) +
  scale_y_continuous(labels = scales::percent, name = "Probability") +
  theme_minimal()

```

## Cool but how does it work? {.smaller}

##### My School: Let's play around

```{r make_km_estimates_by_hand}
#| code-line-numbers: "|3,4|6,7,8,9,10|11,12,13,14,15"
make_km_estimates_by_hand <- function(df, time_var, event_var){
  
  total_at_start <- df |> 
    nrow()
  
  df |> 
    group_by({{ time_var }}) %>% 
    summarise(n_event = sum({{ event_var }}), 
              n_censor = sum({{ event_var }} == 0)) |> 
    mutate(n_risk = lag(total_at_start - cumsum(n_event) - cumsum(n_censor), default = total_at_start),
           prob_repurchase = (n_risk - n_event)/n_risk, 
           estimate = cumprod((n_risk - n_event)/n_risk), 
           std_error = sqrt(cumsum(n_event/(n_risk*(n_risk - n_event)))) * estimate,
           conf_high = estimate + (1.96 * std_error), 
           conf_low = estimate - (1.96 * std_error)) |> 
    select("time" = {{ time_var}}, 
           n_risk, 
           n_event, 
           n_censor, 
           estimate, 
           std_error, 
           conf_high, 
           conf_low)
}
```

```{r}
#| include: false
km_base_as <- make_km_estimates_by_hand(telco_data_clean, tenure, event)
km_base_as
```

## Intuition {.smaller}

The survival probability at time t is the conditional probability of surviving beyond that time, given that an individual has survived to time t - 1.

We can estimate the survival probability by dividing the number of customers who have renewed excluding censoring at that time, divided by the number of customers who had renewed just prior to that time.

The Kaplan-Meier estimate of survival time t is the product of all the conditional probabilities up until time t.

## Comparing the 3 implementations {.smaller}

| Old School: survival package                            | New School: censored package                                            | My School: custom function                       |
|---------------------------|-------------------|--------------------------|
| Just a few lines of code; quick and easy to get started | A little more verbose initially                                         | Dear god, don't ever use this in production code |
|                                                         | Consistency experience fitting, predicting, and the object you get back | Helpful for giving an intuition                  |

## Don't simply exclude censored observations {.smaller}

```{r compare_naive_to_km_estimates_repeat}
#| echo: false
km_naive_estimates_plot
```

It is essential to address **censoring** when you are working with time-to-event data. **Survival analyses** are a use set of tools. There are a few packages for implementing survival analysis in R.

## Special topics

-   competing risks
-   predicting recurrent events
-   synthesizing data

## Additional resources

[Emily Zabor Survival Analysis Tutorial](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Part_1:_Introduction_to_Survival_Analysis)

[Hannah Frick's censored - survival analysis in Tidymodels from rstudio_conf(2022)](https://www.rstudio.com/conference/2022/talks/censored-survival-analysis-in-tidymodels/)

[censored - parsnip extension package](https://censored.tidymodels.org/)

[ggsurvfit package for plotting time-to-event data](http://www.danieldsjoberg.com/ggsurvfit/index.html)

```{r}
#| include: false
# pulled from here
# https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/bs704_survival4.html#:~:text=With%20the%20Kaplan%2DMeier%20approach,the%20actuarial%20life%20table%20approach.
# I don't agree that there are 20 observations at risk at start
# I see 18

simple_df <- tibble(time = c(0,1,2,3,5,6,9,10,11,12,13,14,17,18,19,21,23,24), 
                   event= c(0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,0,1,0))

make_km_estimates_by_hand(simple_df, time, event) |> 
    mutate(estimate = round(estimate,3),
         std_error =round(std_error,3))
```

```{r}
#| include: false
# pulled from here
# https://real-statistics.com/survival-analysis/kaplan-meier-procedure/confidence-interval-for-the-survival-function/
another_simple_df <- tibble(time = c(3,5,8,10,5,5,8,12,15,14,2,11,10,9,12,5,8,11), 
                   event = c(1,1,1,0,1,0,1,1,0,0,1,1,0,0,1,1,1,1))

make_km_estimates_by_hand(another_simple_df, time, event)
```

```{r}
#| include: false
lung <- lung|> 
  mutate(
    status = recode(status, `1` = 0, `2` = 1)
  )
```

```{r}
#| include: false
lung |> 
  filter(status == 1, time < 366)
```

```{r}
#| include: false
1 - (121/228)
```

```{r}
#| include: false
lung |> filter(time == 11)
```

```{r}
#| include: false
naive_estimate_lung <- tibble("total" = nrow(lung), lung |> 
  count(time, status, name = "daily_total")) |> 
  filter(status == 1) |> 
  mutate(running_total = cumsum(daily_total), 
                             naive_estimate = 1 - (running_total/total)) 
```

```{r}
#| include: false
naive_estimate_lung |> 
  #filter(tenure != 0) |>
  ggplot(aes(x = time, y = naive_estimate)) +
  geom_line(color = "blue") +
  #scale_x_date(name = "Signup Date") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Simply removing censored observation over-estimates the survival probability.", 
       subtitle = "These estimates are incorrect.",
       x = "Time since sign up (months)", 
       y = "Percent") +
  theme_minimal()
```
