---
title: "Predicting Time Between Events: Survival Analysis 3 Ways"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
```

## TODOs {.smaller}

-   [x] S hat LaTex formatting
-   [ ] Add median time to churn
-   [ ] Add motivation for censored
-   [x] Clean up dead code - lung and simple dfs at end
-   [ ] Finish speaker notes
-   [x] Give slide titles and plot titles another pass

## Who are you?

![](https://media.giphy.com/media/3ohs4lOlH8GhRtb6QU/giphy.gif)

## I'm Andrea. Hi! {.smaller}

::: columns
::: {.column width="60%"}
![](talk_images/sorcinelli_headshot.jpg)
:::

::: {.column width="40%"}
```{r}
#| eval: false
andrea |> 
  study(cognition_and_perception) |> 
  work(brain_pop, warby_parker)
```
:::
:::

## How I think about code in talks

There is code in this talk.

Don't worry about the details.

*Really.*

## My goal is for you to leave with an understanding of

-   The kinds of situations that call for survival analysis\
-   Why it is important not to treat censored observations as missing\
-   There are different packages for implementing survival analysis in R\
-   An intuition for how survival analysis works

## Back to how I think about code in talks

Don't worry about the details of the code.

Focus on the the high-level. It's all on GitHub.

## Predicting time between sign up and churn {.smaller}

Imagine you work for a communication service provider like Xfinity or Waitsfield Telecom. A big part of your business is going to be focused on understanding renewals.

Who keeps their contract and who **cancels their contract or churns**? Ideally, you'd like to be able to estimate a probability that someone will remain a customer.\
\
\
Data from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) used throughout.

## Some customers signed up a long time ago; others only recently

```{r}
#| include: false
fake_data <- tibble(customer = c("Alice", "Bob", "Charlie", "Dave", "Erin"),
               signup_date = lubridate::ymd(c("2019-01-01", "2020-01-01", "2021-01-01", "2022-01-01", "2022-07-01")),
               churn_date = lubridate::ymd(c("2021-03-01", "2022-02-01", NA, NA, "2023-01-01")),
               tenure =  round(as.numeric(as.duration(churn_date - signup_date), 'months'),2),
         is_churned = as.numeric(!is.na(churn_date)),
         cohort = year(signup_date))
```

```{r}
#| echo: false
fake_data %>%
  ggplot(.,  aes(y = fct_rev(customer))) +
  geom_point(aes(x = signup_date, color = "blue"), size = 3) +
  geom_point(aes(x = churn_date, color = "red"), size = 3) +
  geom_segment(aes(x = signup_date,
                   y = customer,
                   xend = coalesce(churn_date, lubridate::ymd("2023-04-01")),
                   yend = customer),
               linewidth = 1) +
  scale_colour_manual(breaks = c("blue", "red"),
                      labels = c("Signup Date", "Churn Date"),
                      values = c("blue", "red")) +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  theme(panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black")) +
  theme(legend.position = "none") +
  labs(x = "Year",
       y = "Customer",
       title = "Time from sign up to churn",
       subtitle = "We have observed churn for some customers. \nWe have not for others.",
       caption = "Charlie and Dave are censored") +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

::: notes
Here's some fake data that demonstrates a common problem that arises.

In the top row, we see customer Alice. Alice signed up for an account in 2019 and she cancelled or churned in 2021.

And in the next row, we see Bob. And Bob signed up for an account in 2020 and cancelled or churned in 2022.

And then we see Charlie in the third row. And Charlie signed up in 2021 but Charlie still has his contract. So we have not observed Charlie cancel or churn yet. The same thing for Dave in the fourth row.
:::

## Charlie and Dave are censored {.smaller}

We know Charlie and Dave's starting point - their signup date, but we have not observed their ending date - their churn date.

These are **censored** observations.

It can be tempting to treat these observations as missing. But they're not.\
\
We have **partial** information for Charlie and Dave.

Excluding them from the analysis will result in incorrect estimates of when customers churn.

::: notes
This phenomenon is known as censoring. Censoring means that we know something about a individual's starting point, in this case, their sign up date but we are missing information about their ending point, their cancellation or churn date.

A common response is to filter censored observations like these out. In other words to treat them as missing. But they're not missing. Indeed, we have partial infortmation for them.
:::

```{r read_data}
#| include: false
# download from https://www.kaggle.com/datasets/blastchar/telco-customer-churn and put in a data folder
telco_data <- read.csv("data/WA_Fn-UseC_-Telco-Customer-Churn.csv") 
```

```{r data_cleaning}
#| include: false
 telco_data_clean <- telco_data |> 
  janitor::clean_names()  |>
  mutate(event = if_else(churn == "Yes", 1, 0)) |> 
  select(customer_id, tenure, churn, event,everything())
```

## We cannot simply remove censored observations

```{r calc_naive_estimates}
#| echo: false
naive_estimate <- tibble(total = nrow(telco_data_clean), 
                         telco_data_clean |> 
                           count(tenure, event, name = "monthly_total")) |> 
  filter(event == 1) |> 
  mutate(running_total = cumsum(monthly_total), 
         naive_estimate = 1 - (running_total/total))
```

```{r plot_naive_estimates}
#| echo: false
naive_estimate |> 
  ggplot(aes(x = tenure, y = naive_estimate)) +
  geom_line(color = "red") +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Simply removing censored observations overestimates the \nsurvival probability.", 
       subtitle = "These estimates are incorrect.",
       x = "Time since sign up (months)", 
       y = "Percent") +
  theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

::: notes
What happens when we filter censored observations out is that we can construct naive estimates of the survival probability. But these estimates are incorrect.

This naive estimate is 1 minus the running total of individuals who have churned divided by the total n in the sample.

And this over-estimates the survival probability.

Individuals who are censored only contribute information for some of the time and then they need to fall out of the risk set. But if we ignore censoring, it treats these indivudals as part of the risk set for the entire time when they are not eligible to be.
:::

## We encounter censored data all the time {.smaller}

-   Time from disease onset to death
-   Time from machine creation to failure
-   Time from order to re-order
-   Time from sign up to churn

If you are asking yourself, *"how long until?"* or *"has it happened yet?"* you likely have censored data.

\
These are all examples of **right censored** data but there are other kinds.

## Survival analyses help us address censored data

Survival analysis provide techniques for us to take into account censoring in our data. Two common methods:

-   Kaplan Meier Estimates to estimate survival probabilities
-   Cox Proportional Hazards to quantify effects between one or more variables and estimate the probability that an individual experiences the event of interest during a small time interval

::: notes
I am going to focus on these because they map onto two fundamental functions of interest: - the **survival probability** or the probability that an individual survives past time t - the **hazard function** or the probability that an individual experiences event during a small time interval given that the individual has survived up until the start of that interval
:::

## What does time-to-event data look like? {.smaller}

All you need to get started are two variables:

-   time variable (units don't matter)
-   event marker (1 = event observed; 0 = censored)

```{r time_to_event_data}
telco_data_clean |> 
  select(tenure, event) |> 
  head(10)
```

## What is the probability of surviving past a given time? {.smaller}

##### Old School: survival package

```{r km_base_surv}
library(survival)
km_base_surv <- survfit(Surv(tenure, event) ~ 1, 
                      data = telco_data_clean) |> 
  broom::tidy(km)

km_base_surv
```

## What is the probability of surviving past a given time? {.smaller}

```{r plot_km_base_surv}
#| echo: false
km_base_surv %>%
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_line(aes(x = time, y = estimate), color = "blue") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = "Probability of survival beyond a given time",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

**Interpretation:** The survival probability at time t is the conditional probability of surviving beyond that time, given that an individual has survived to time t - 1.

## Comparing Kaplan Meier to naive estimates

```{r compare_naive_to_km_estimates}
#| echo: false
km_naive_estimates_plot <- ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_line(data = km_base_surv, aes(x = time, y = estimate, color="Kaplan Meier")) +
  geom_line(data = naive_estimate, aes(x = tenure, y = naive_estimate, color = "Naive")) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  scale_color_manual(name="Estimate Type",
                     breaks=c("Kaplan Meier", "Naive"),
                     values=c("Kaplan Meier" = "blue", "Naive"="red")) +
  labs(title = "Probability of survival beyond a given time",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))

km_naive_estimates_plot
```

## Do survival probabilities differ depending on the type of internet service? {.smaller}

##### Old School: survival package

```{r km_internet_service_surv}
km_internet_service_surv <- survfit(Surv(tenure, event) ~ internet_service, 
                              data = telco_data_clean) |> 
  broom::tidy(km) 

km_internet_service_surv
```

## Do survival probabilities differ depending on the type of internet service?

```{r plot_km_internet_service_surv}
#| echo: false
km_internet_service_surv |> 
  separate(strata, into = c("internet_service"), sep=", ") |> 
  mutate_at(c("internet_service"), ~ str_trim(str_remove(.x, '^.*='), side = "right")) |> 
  ggplot() +
  geom_hline(yintercept = 0.5, color = "gray", lwd = 0.5) +
  geom_ribbon(aes(x = time,
                  ymin = conf.low,
                  ymax = conf.high,
                  fill = internet_service),
              alpha = 0.25) +
  geom_line(aes(x = time, y = estimate, color = internet_service)) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(0,80, by = 12)) +
  labs(title = "Probability of survival by internet service type",
       subtitle = "",
       caption = "",
       x = "Time since sign up (months)",
       y = "Probability") +
    theme_minimal() +
    theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 15))
```

## How can we quantify the difference between groups? {.smaller}

##### Old School: survival package

```{r cox_ph_internet_service_surv}
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean) 

cox_ph_internet_service_surv
```

## How can we quantify the difference between groups? {.smaller}

##### New School: censored package

```{r cox_ph_internet_service_tidy}
library(parsnip)
library(censored)

cox_ph_internet_service_tidy <- proportional_hazards()  |>  
  set_engine("survival") |>  
  set_mode("censored regression") |>  
  fit(
    Surv(tenure, event) ~ internet_service,
    data = telco_data_clean
)

cox_ph_internet_service_tidy
```

## What *exactly* is a hazard of churning? {.smaller}

**Interpretation:** We are often interested in a **hazard ratio**.

A **hazard** represents the probability of churning in time t + 1 if a customer has not churned as of time t.

The **hazard ratio** therefore represents the ratio of hazards between two groups at any particular time.

-   exponentiate the regression parameter β to get the hazard ratio

-   β \< 0: HR \< 1: reduced hazard of churn

-   β \> 0: HR \> 1: increased hazard of churn

## What *exactly* is a hazard of churning? {.smaller}

So the HR = 2.21 means that 2.21 times as many customers with Fiber Optic are churning than those with DSL at any given time. **In other words, DSL has a significantly lower hazard of churn than Fiber Optic.**

And the HR = 0.40 implies that 0.40 times as many customers no internet service are churning as those with with DSL at any given time. **In other words, customers with DSL have a significantly higher hazard of churn than those with no internet service.**

```{r}
#| echo: false
cox_ph_internet_service_tidy
```

## Instantaneous hazard of churning

```{r extract_hazards_and_plot}
#| echo: false
cox_ph_internet_service_surv <- coxph(Surv(tenure, event) ~ internet_service,
      data = telco_data_clean)

pred_df <- cross_df(
  list(strata = unique(telco_data_clean[!is.na(telco_data_clean[["internet_service"]]), ][["internet_service"]]),
       tenure = 0:72,
       event = 1)) |> 
  rename("internet_service" = strata) |> 
  arrange(internet_service, tenure)

pred_df <- pred_df |>
  mutate(estimate = predict(cox_ph_internet_service_surv, pred_df, type = "survival")) |>
  rename(time = tenure)

pred_df |>  
  group_by(internet_service) |> 
  mutate(hazard = c(NA, -diff(estimate)) / estimate) |> 
  ungroup() |> 
  ggplot(aes(x = time, y = hazard, color = internet_service)) +
  geom_line() +
  scale_x_continuous(name = "Time since sign up",
                     breaks = seq(0, 72, by = 6)) +
  scale_y_continuous(labels = scales::percent, name = "Probability") +
  theme_minimal()

```

## Cool but how does it work?

![](https://media.giphy.com/media/yj5oYHjoIwv28/giphy.gif)

## Intuition for Kaplan Meier estimator {.smaller}

$$\widehat{S}(t)∏_{i: t_{i}≤t}\left(1 - \frac{d_{i}}{n_{i}}\right)$$

-   Survival probability at time t is conditional probability of surviving beyond t, given survival to t - 1.

-   Divide the number of customers who have renewed excluding censoring at time t by the number of customers who had renewed at time t -1.

-   The Kaplan-Meier estimate of survival time t is product of all the conditional probabilities up until t.

::: notes
The survival probability at time t is the conditional probability of surviving beyond that time, given that an individual has survived to time t - 1.

We can estimate the survival probability by dividing the number of customers who have renewed excluding censoring at that time, divided by the number of customers who had renewed just prior to that time.

The Kaplan-Meier estimate of survival time t is the product of all the conditional probabilities up until time t.
:::

## Intuition for Kaplan Meier estimator {.smaller}

##### My School: Let's play around

```{r make_km_estimates_by_hand}
#| code-line-numbers: "|3,4|6,7,8,9,10,11|12,13,14,15"
make_km_estimates_by_hand <- function(df, time_var, event_var){
  
  total_at_start <- df |> 
    nrow()
  
  df |> 
    group_by({{ time_var }}) %>% 
    summarise(
      n_event = sum({{ event_var }}), 
      n_censor = sum({{ event_var }} == 0)
      ) |> 
    mutate(
      n_risk = lag(total_at_start - cumsum(n_event) - cumsum(n_censor), default = total_at_start),
      prob_repurchase = (n_risk - n_event)/n_risk, 
      estimate = cumprod((n_risk - n_event)/n_risk), 
      std_error = sqrt(cumsum(n_event/(n_risk*(n_risk - n_event)))) * estimate,
      conf_high = estimate + (1.96 * std_error), 
      conf_low = estimate - (1.96 * std_error)
      ) |> 
    select(
      "time" = {{ time_var}}, 
       n_risk, 
       n_event, 
       n_censor, 
       estimate, 
       std_error, 
       conf_high, 
       conf_low)
}
```

## Intuition for Kaplan Meier estimator {.smaller}

##### My School: What does that output look like?

```{r}
km_base_as <- make_km_estimates_by_hand(telco_data_clean, tenure, event)
km_base_as
```

## Comparing the 3 implementations {.smaller}

| Old School: survival package                            | New School: censored package                                            | My School: custom function                       |
|---------------------------|-------------------|--------------------------|
| Just a few lines of code; quick and easy to get started | A little more verbose initially                                         | Dear god, don't ever use this in production code |
|                                                         | Consistenct experience fitting, predicting, and the object you get back | Helpful for giving an intuition                  |

## Don't simply exclude censored observations {.smaller}

```{r compare_naive_to_km_estimates_repeat}
#| echo: false
km_naive_estimates_plot
```

It is essential to address **censoring** when you are working with time-to-event data. **Survival analyses** are a use set of tools. There are a few packages for implementing survival analysis in R.

## Special topics {.smaller}

-   competing risks
    -   occurs when there are multiple possible alternative events
        -   death from disease
        -   death from other causes
        -   treatment response
-   predicting recurrent events
    -   occurs when events may be repeated
        -   disease recurrence
        -   repurchases
-   interval censored data
    -   occurs when we know that an event took place during an interval but not the exact time
        -   dentistry
        -   ophthalmology
-   synthesizing data
    -   method for simulating time-to-event data without specifying a particular distribution for the baseline hazard function

## Additional resources

[Emily Zabor Survival Analysis Tutorial](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Part_1:_Introduction_to_Survival_Analysis)

[Hannah Frick's censored - survival analysis in Tidymodels from rstudio_conf(2022)](https://www.rstudio.com/conference/2022/talks/censored-survival-analysis-in-tidymodels/)

[censored - parsnip extension package providing engines for censored regression and survival analysis](https://censored.tidymodels.org/)

[Red Door Analytics Survival Analysis Tutorials](https://reddooranalytics.se/resources/tutorials/)

[coxed package - for simulating survival data](https://cran.r-project.org/web/packages/coxed/vignettes/simulating_survival_data.html)

[ggsurvfit package - for plotting time-to-event data including risk tables](http://www.danieldsjoberg.com/ggsurvfit/index.html)
